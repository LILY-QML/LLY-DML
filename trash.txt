# main.py

import json
import os
import threading
import numpy as np
import logging
from qiskit import ClassicalRegister
from tqdm import tqdm
from datetime import datetime

from module.circuit import Circuit
from module.optimizer import (
    Optimizer,
    OptimizerWithMomentum,
    AdamOptimizer,
    GeneticOptimizer,
    PSOOptimizer,
    BayesianOptimizer,
    SimulatedAnnealingOptimizer,
    QuantumNaturalGradientOptimizer
)
from module.visual import Visual

# ------------------------------
# Logging-Konfiguration
# ------------------------------
if not os.path.exists('var'):
    os.makedirs('var')

logging.basicConfig(
    filename='var/log.logdb',
    level=logging.DEBUG,  # Log-Level auf DEBUG setzen
    format='%(asctime)s - %(levelname)s - %(message)s'
)

# ------------------------------
# DataCollector Klasse
# ------------------------------
class DataCollector:
    def __init__(self, data_file='var/train.json'):
        logging.debug("Initialisiere DataCollector")
        self.data_file = data_file
        self.lock = threading.Lock()
        self.data = self.load_data()

    def load_data(self):
        logging.debug("Lade Daten aus train.json")
        if os.path.exists(self.data_file):
            with open(self.data_file, 'r') as f:
                data = json.load(f)
                logging.info("Trainingsdaten geladen.")
                return data
        else:
            # Initialisiere mit leerer Struktur und schreibe Datum
            logging.info("train.json nicht gefunden. Initialisiere neue Datenstruktur.")
            data = {
                "creation_date": datetime.now().isoformat(),
                "training_matrix": None,
                "optimizers": {}
            }
            self.data = data
            self.save_data()
            return data

    def save_data(self):
        logging.debug("Speichere Daten in train.json")
        logging.debug(f"Daten, die gespeichert werden: {self.data}")
        with self.lock:
            try:
                with open(self.data_file, 'w') as f:
                    json.dump(self.data, f, indent=4)
                    logging.info("Trainingsdaten gespeichert.")
            except Exception as e:
                logging.error(f"Fehler beim Speichern der Daten: {e}")
                print(f"Fehler beim Speichern der Daten: {e}")

    def set_training_matrix(self, matrix):
        logging.debug("Setze Trainingsmatrix")
        logging.debug(f"Typ der Trainingsmatrix: {type(matrix)}")
        logging.debug(f"Inhalt der Trainingsmatrix: {matrix}")
        with self.lock:
            self.data["training_matrix"] = matrix
            self.save_data()
            logging.info("Trainingsmatrix gesetzt.")

    def get_training_matrix(self):
        logging.debug("Hole Trainingsmatrix")
        return self.data.get("training_matrix")

    def add_optimizer_result(self, optimizer_name, result):
        logging.debug(f"Füge Optimierergebnis für {optimizer_name} hinzu")
        with self.lock:
            result['training_date'] = datetime.now().isoformat()  # Trainingsdatum hinzufügen
            self.data["optimizers"][optimizer_name] = result
            self.save_data()
            logging.info(f"Ergebnis für Optimierer '{optimizer_name}' gespeichert.")

    def get_optimizer_result(self, optimizer_name):
        logging.debug(f"Hole Ergebnis für Optimierer {optimizer_name}")
        return self.data["optimizers"].get(optimizer_name)

    def get_completed_optimizers(self):
        logging.debug("Hole Liste der abgeschlossenen Optimierer")
        return list(self.data["optimizers"].keys())

# ------------------------------
# TrainingManager Klasse
# ------------------------------
class TrainingManager:
    def __init__(self, optimizers, training_matrix, config, data_collector, activation_matrix_idx, target_state):
        logging.debug("Initialisiere TrainingManager")
        self.optimizers = optimizers
        self.training_matrix = training_matrix
        self.config = config
        self.data_collector = data_collector
        self.activation_matrix_idx = activation_matrix_idx
        self.target_state = target_state

        self.optimizer_classes = {
            "Basic Gradient Descent": Optimizer,
            "Momentum": OptimizerWithMomentum,
            "Adam": AdamOptimizer,
            "Genetic Algorithm": GeneticOptimizer,
            "Particle Swarm Optimization": PSOOptimizer,
            "Bayesian Optimization": BayesianOptimizer,
            "Simulated Annealing": SimulatedAnnealingOptimizer,
            "Quantum Natural Gradient": QuantumNaturalGradientOptimizer
        }

    def run_all_optimizers(self):
        logging.info("Starte run_all_optimizers()")
        threads = []
        for optimizer_name in self.optimizers:
            logging.info(f"Starte Thread für Optimierer: {optimizer_name}")
            thread = threading.Thread(target=self.run_single_optimizer, args=(optimizer_name,))
            threads.append(thread)
            thread.start()

        for thread in threads:
            thread.join()
        logging.info("Alle Optimierer abgeschlossen.")

    def run_single_optimizer(self, optimizer_name):
        logging.info(f"Starte run_single_optimizer() für {optimizer_name}")
        optimizer_class = self.optimizer_classes.get(optimizer_name)

        if optimizer_class is None:
            logging.warning(f"Optimizer {optimizer_name} not recognized, skipping...")
            return

        logging.info(f"Running optimizer: {optimizer_name} for Activation Matrix {self.activation_matrix_idx + 1}")

        try:
            # Initialisiere Circuit
            circuit = Circuit(
                qubits=self.config['qubits'],
                depth=self.config['depth'],
                training_phases=self.training_matrix,
                activation_phases=self.config['activation_matrices'][self.activation_matrix_idx],
                shots=self.config['shots']
            )

            # Sicherstellen, dass klassische Bits vorhanden sind
            if len(circuit.circuit.clbits) == 0:
                circuit.circuit.add_register(ClassicalRegister(circuit.circuit.num_qubits))
                circuit.circuit.measure(range(circuit.circuit.num_qubits), range(circuit.circuit.num_qubits))

            # Instanziiere den Optimizer mit den entsprechenden Parametern
            optimizer = self.initialize_optimizer(optimizer_name, optimizer_class, circuit)

            # Optimierung durchführen mit Fortschrittsanzeige
            with tqdm(total=self.config['max_iterations'], desc=f"{optimizer_name}", position=0, leave=True) as pbar:
                def progress_callback(*args, **kwargs):
                    pbar.update(1)

                optimized_phases, losses = optimizer.optimize(progress_callback=progress_callback)

            # Aktualisiere die Trainingsphasen im Circuit
            circuit.training_phases = optimized_phases

            # Circuit mit optimierten Phasen erneut ausführen
            final_result = circuit.run()
            final_counts = final_result.get_counts()
            final_distribution = self.get_distribution(final_counts)

            # Wahrscheinlichkeit des Zielzustands aufzeichnen
            initial_counts = self.config['initial_counts_map'][str(self.activation_matrix_idx)]
            initial_probability = initial_counts.get(self.target_state, 0) / sum(initial_counts.values())
            final_probability = final_distribution.get(self.target_state, 0) / sum(final_distribution.values())

            # Ergebnisse vorbereiten
            result = {
                "Activation Matrix": self.activation_matrix_idx + 1,
                "Target State": self.target_state,
                "Optimizer": optimizer_name,
                "Initial Probability": initial_probability,
                "Final Probability": final_probability,
                "Losses": losses,
                "Initial Counts": initial_counts,
                "Final Counts": final_counts,
                "Optimized Phases": optimized_phases
            }

            # Ergebnisse speichern
            self.data_collector.add_optimizer_result(optimizer_name, result)

            logging.info(f"Optimizer {optimizer_name} completed.")

        except Exception as e:
            logging.error(f"Error running optimizer {optimizer_name}: {e}")
            print(f"Error running optimizer {optimizer_name}: {e}")

    def initialize_optimizer(self, optimizer_name, optimizer_class, circuit):
        logging.debug(f"Initialisiere Optimierer: {optimizer_name}")
        if optimizer_name == "Genetic Algorithm":
            optimizer = optimizer_class(
                circuit,
                self.target_state,
                self.config['learning_rate'],
                self.config['max_iterations'],
                population_size=self.config.get('population_size', 20),
                mutation_rate=self.config.get('mutation_rate', 0.1)
            )
        elif optimizer_name == "Particle Swarm Optimization":
            optimizer = optimizer_class(
                circuit,
                self.target_state,
                self.config['learning_rate'],
                self.config['max_iterations'],
                num_particles=self.config.get('num_particles', 3),
                inertia=self.config.get('inertia', 0.5),
                cognitive=self.config.get('cognitive', 1.5),
                social=self.config.get('social', 1.5)
            )
        elif optimizer_name == "Bayesian Optimization":
            phase_shape = np.array(circuit.training_phases).shape
            flat_phases_size = np.prod(phase_shape)
            bounds = [(0, 2 * np.pi)] * flat_phases_size
            optimizer = optimizer_class(
                circuit,
                self.target_state,
                self.config['learning_rate'],
                self.config['max_iterations'],
                bounds=bounds
            )
        elif optimizer_name == "Simulated Annealing":
            optimizer = optimizer_class(
                circuit,
                self.target_state,
                self.config['learning_rate'],
                self.config['max_iterations'],
                initial_temperature=self.config.get('initial_temperature', 1.0),
                cooling_rate=self.config.get('cooling_rate', 0.99)
            )
        elif optimizer_name == "Quantum Natural Gradient":
            num_parameters = self.config['depth'] * 3 * self.config['qubits']
            fisher_information_matrix = np.identity(num_parameters).tolist()
            optimizer = optimizer_class(
                circuit,
                self.target_state,
                self.config['learning_rate'],
                self.config['max_iterations'],
                fisher_information_matrix=fisher_information_matrix
            )
        else:
            # Basic Gradient Descent, Momentum, Adam
            optimizer = optimizer_class(
                circuit,
                self.target_state,
                self.config['learning_rate'],
                self.config['max_iterations']
            )
        return optimizer

    def get_distribution(self, counts):
        total = sum(counts.values())
        distribution = {state: count / total for state, count in counts.items()}
        return distribution

# ------------------------------
# DML Hauptklasse
# ------------------------------
class DML:
    def __init__(self, json_path='var/data.json'):
        logging.debug("Initialisiere DML-Klasse")
        self.json_path = json_path
        self.config = self.load_config()
        self.data_collector = DataCollector('var/train.json')
        self.activation_matrix_idx = 0  # Annahme: Eine Aktivierungsmatrix für Einfachheit

        # Initialisiere initial_counts_map, falls nicht vorhanden
        if "initial_counts_map" not in self.config:
            self.config["initial_counts_map"] = {}
            self.save_config()

    def load_config(self):
        logging.debug("Lade Konfigurationsdaten aus data.json")
        if not os.path.exists(self.json_path):
            logging.error(f"JSON file not found at path: {self.json_path}")
            raise FileNotFoundError(f"JSON file not found at path: {self.json_path}")

        with open(self.json_path, 'r') as f:
            logging.info("Konfigurationsdaten geladen.")
            return json.load(f)

    def save_config(self):
        logging.debug("Speichere Konfigurationsdaten in data.json")
        with open(self.json_path, 'w') as f:
            json.dump(self.config, f, indent=4)
            logging.info("Konfigurationsdaten gespeichert.")

    def generate_random_training_phases(self):
        logging.debug("Generiere zufällige Trainingsphasen")
        qubits = self.config['qubits']
        depth = self.config['depth']
        np_matrix = np.random.uniform(0, 2 * np.pi, (depth * 3, qubits))
        # Konvertiere die Matrix zu einer Liste von Listen aus Python-Floats
        training_matrix = np_matrix.astype(float).tolist()
        return training_matrix

    def create_training_matrix(self):
        logging.info("Starte create_training_matrix()")
        try:
            training_matrix = self.generate_random_training_phases()
            self.data_collector.set_training_matrix(training_matrix)
            logging.info("Trainingsmatrix erstellt und gespeichert.")
            print("Training matrix has been generated and saved to var/train.json.")
        except Exception as e:
            logging.error(f"Fehler beim Erstellen der Trainingsmatrix: {e}")
            print(f"Fehler beim Erstellen der Trainingsmatrix: {e}")
        logging.info("Beende create_training_matrix()")

    def initialize_circuit(self, activation_matrix_idx=0):
        logging.info("Starte initialize_circuit()")
        try:
            qubits = self.config['qubits']
            depth = self.config['depth']
            activation_phases = self.config['activation_matrices'][activation_matrix_idx]
            training_phases = self.data_collector.get_training_matrix()
            if training_phases is None:
                raise ValueError("Training matrix is None.")

            circuit = Circuit(
                qubits=qubits,
                depth=depth,
                training_phases=training_phases,
                activation_phases=activation_phases,
                shots=self.config['shots']
            )

            # Circuit ausführen, um initiale Counts zu erhalten
            initial_result = circuit.run()
            initial_counts = initial_result.get_counts()
            self.config.setdefault("initial_counts_map", {})
            self.config["initial_counts_map"][str(activation_matrix_idx)] = initial_counts
            self.save_config()

            # Bestimme den Zielzustand
            total_counts = sum(initial_counts.values())
            initial_probabilities = {state: count / total_counts for state, count in initial_counts.items()}
            target_state = max(initial_probabilities, key=initial_probabilities.get)
            logging.info(f"Initial most likely target state: {target_state}")
            print(f"Initial most likely target state: {target_state}")

            logging.info("initialize_circuit() erfolgreich abgeschlossen.")
            return target_state
        except Exception as e:
            logging.error(f"Fehler bei initialize_circuit: {e}")
            print(f"Fehler bei initialize_circuit: {e}")
            raise

    def display_status(self):
        logging.debug("Zeige aktuellen Status an")
        print("\nCurrent Status:")
        training_matrix_status = "✔" if self.data_collector.get_training_matrix() else "✘"
        print(f"Training Matrix: {training_matrix_status}")

        completed_optimizers = self.data_collector.get_completed_optimizers()
        print("Optimizers:")
        for optimizer_name in self.config['optimizers']:
            status = "✔" if optimizer_name in completed_optimizers else "✘"
            print(f"  {optimizer_name}: {status}")

    def run_optimizers(self, optimizers_to_run, activation_matrix_idx, target_state):
        logging.info("Starte run_optimizers()")
        try:
            training_manager = TrainingManager(
                optimizers=optimizers_to_run,
                training_matrix=self.data_collector.get_training_matrix(),
                config=self.config,
                data_collector=self.data_collector,
                activation_matrix_idx=activation_matrix_idx,
                target_state=target_state
            )
            training_manager.run_all_optimizers()
            logging.info("run_optimizers() erfolgreich abgeschlossen.")
        except Exception as e:
            logging.error(f"Fehler bei run_optimizers: {e}")
            print(f"Fehler bei run_optimizers: {e}")

    def generate_report(self):
        logging.info("Starte generate_report()")
        # Sammle alle Ergebnisse
        results = []
        for optimizer_name in self.config['optimizers']:
            result = self.data_collector.get_optimizer_result(optimizer_name)
            if result:
                results.append(result)

        if not results:
            logging.warning("No results to generate report.")
            print("No results to generate report.")
            return

        visual = Visual(
            results=results,
            target_states=[result['Target State'] for result in results],
            initial_training_phases=[self.data_collector.get_training_matrix()] * len(results),
            activation_matrices=self.config['activation_matrices'],
            circuits=[],  # Annahme: Keine Circuits für den Report benötigt
            num_iterations=self.config['max_iterations'],
            qubits=self.config['qubits'],
            depth=self.config['depth'],
            additional_data=self.config
        )

        visual.generate_report("QuantumCircuitReport.pdf")
        logging.info("Report has been successfully generated.")
        print("Report has been successfully generated.")
        logging.info("Beende generate_report()")

    def run_specific_optimizer(self, optimizer_initial, activation_matrix_idx, target_state):
        logging.info("Starte run_specific_optimizer()")
        # Finde Optimierer, die mit dem eingegebenen Anfangsbuchstaben beginnen
        matching_optimizers = [opt for opt in self.config['optimizers'] if opt.lower().startswith(optimizer_initial.lower())]
        if not matching_optimizers:
            logging.warning("No optimizer found with that initial.")
            print("No optimizer found with that initial.")
            return

        optimizer_name = matching_optimizers[0]
        if optimizer_name in self.data_collector.get_completed_optimizers():
            print(f"Optimizer {optimizer_name} has already been run and will be overwritten.")

        self.run_optimizers([optimizer_name], activation_matrix_idx, target_state)
        logging.info("Beende run_specific_optimizer()")

    def run(self):
        logging.info("Starte Hauptprogramm")
        while True:
            self.display_status()
            print("\nOptions:")
            print("1. All - Run all optimizers with a new training matrix")
            print("2. Remaining - Run remaining optimizers with the last training matrix")
            print("3. Report - Generate report based on existing data")
            print("4. Run a specific optimizer")
            print("5. Exit")
            choice = input("Enter your choice: ").strip()

            logging.info(f"Benutzer hat Option {choice} gewählt.")

            if choice == '1' or choice.lower() == 'all':
                logging.info("Option 1 gewählt: All")
                self.create_training_matrix()
                target_state = self.initialize_circuit()
                self.run_optimizers(self.config['optimizers'], self.activation_matrix_idx, target_state)
            elif choice == '2' or choice.lower() == 'remaining':
                logging.info("Option 2 gewählt: Remaining")
                if not self.data_collector.get_training_matrix():
                    print("No existing training matrix found. Please run option 1 first.")
                    continue
                target_state = self.initialize_circuit()
                remaining_optimizers = [opt for opt in self.config['optimizers'] if opt not in self.data_collector.get_completed_optimizers()]
                if not remaining_optimizers:
                    print("All optimizers have already been run.")
                    continue
                self.run_optimizers(remaining_optimizers, self.activation_matrix_idx, target_state)
            elif choice == '3' or choice.lower() == 'report':
                logging.info("Option 3 gewählt: Report")
                self.generate_report()
            elif choice == '4' or choice.lower() == 'specific':
                logging.info("Option 4 gewählt: Run a specific optimizer")
                optimizer_initial = input("Enter the initial of the optimizer you want to run: ").strip()
                if not optimizer_initial:
                    print("No initial entered.")
                    continue
                if not self.data_collector.get_training_matrix():
                    print("No existing training matrix found. Please run option 1 first.")
                    continue
                target_state = self.initialize_circuit()
                self.run_specific_optimizer(optimizer_initial, self.activation_matrix_idx, target_state)
            elif choice == '5' or choice.lower() == 'exit':
                logging.info("Option 5 gewählt: Exit")
                print("Exiting.")
                break
            else:
                logging.warning("Ungültige Auswahl getroffen")
                print("Invalid choice. Please try again.")

# ------------------------------
# Main Execution
# ------------------------------
if __name__ == "__main__":
    logging.info("Programmstart")
    # Stelle sicher, dass das Verzeichnis 'var' existiert
    if not os.path.exists('var'):
        os.makedirs('var')

    # Pfad zur JSON-Konfigurationsdatei
    json_path = os.path.join('var', 'data.json')

    # Erstelle und führe die DML-Instanz aus
    dml = DML(json_path)
    dml.run()
    logging.info("Programmende")
