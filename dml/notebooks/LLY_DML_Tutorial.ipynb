{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLY-DML: Quantum Circuit Optimization Tutorial\n",
    "\n",
    "<div style=\"text-align: center; padding: 10px; margin: 10px;\">\n",
    "    <h3>LILY Quantum Machine Learning - Differentiable Machine Learning</h3>\n",
    "    <p style=\"font-style: italic;\">Ein Tutorial zur Quantenoptimierung mit L-Gates</p>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "## Einführung\n",
    "\n",
    "Das LLY-DML-Framework ermöglicht die Optimierung von Quantum Circuits mit einem speziellen Fokus auf L-Gate-Strukturen. Dieses Notebook führt Sie durch den Prozess der:\n",
    "\n",
    "1. Erstellung eines Quantum Circuits mit L-Gates\n",
    "2. Parametrisierung des Circuits mit trainierbaren Parametern\n",
    "3. Optimierung dieser Parameter mit dem Adam-Algorithmus\n",
    "4. Analyse und Visualisierung der Optimierungsergebnisse\n",
    "\n",
    "## Was sind L-Gates?\n",
    "\n",
    "Die L-Gate-Struktur ist eine spezielle Sequenz von Quantengates, die für Quantenmaschinelles Lernen optimiert wurde:\n",
    "\n",
    "```\n",
    "TP0 → IP0 → H → TP1 → IP1 → H → TP2 → IP2\n",
    "```\n",
    "\n",
    "Wobei:\n",
    "- **TP** = Trainingsphase (P-Gate mit trainierbaren Parametern)\n",
    "- **IP** = Inputphase (P-Gate mit festen Inputparametern)\n",
    "- **H** = Hadamard-Gate (festes Gate ohne Parameter)\n",
    "\n",
    "Diese Struktur wird für jeden Qubit und in der festgelegten Tiefe (Depth) wiederholt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup und Bibliotheken importieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abhängigkeiten importieren\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "# Importiere Qiskit für Visualisierung\n",
    "from qiskit import QuantumCircuit\n",
    "from qiskit.visualization import circuit_drawer\n",
    "\n",
    "# Konfiguriere Pfade für Module-Imports\n",
    "notebook_dir = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "project_root = os.path.dirname(os.path.dirname(notebook_dir))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# Konfiguriere Logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(\"DML-Tutorial\")\n",
    "\n",
    "# Setze Matplotlib-Stil\n",
    "plt.style.use('ggplot')\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "logger.info(\"Setup abgeschlossen. LLY-DML Tutorial bereit.\")\n",
    "\n",
    "# Zeige die Verzeichnisstruktur\n",
    "print(f\"Notebooks-Verzeichnis: {notebook_dir}\")\n",
    "print(f\"Projekt-Root: {project_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Importiere LLY-DML Module\n",
    "\n",
    "Wir importieren die für das Tutorial benötigten Module aus dem LLY-DML Framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importiere Circuit und Optimizer aus dem DML-Framework\n",
    "from dml.module.src.circuit import Circuit\n",
    "from dml.module.optimizers.adam_optimizer import AdamOptimizer\n",
    "\n",
    "# Überprüfe, ob die Importe erfolgreich waren\n",
    "print(f\"Circuit-Klasse erfolgreich importiert: {Circuit.__name__}\")\n",
    "print(f\"Adam-Optimizer erfolgreich importiert: {AdamOptimizer.__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Erstellen eines Quantum Circuits mit L-Gates\n",
    "\n",
    "Hier erstellen wir einen Circuit mit:\n",
    "- 5 Qubits\n",
    "- Tiefe (Depth) 3 (d.h. 3 L-Gates pro Qubit)\n",
    "\n",
    "Jedes L-Gate besteht aus mehreren Phasen-Gates und Hadamard-Gates in einer bestimmten Reihenfolge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter für den Circuit\n",
    "qubits = 5\n",
    "depth = 3\n",
    "shots = 1024\n",
    "\n",
    "# Erstelle einen Quantum Circuit\n",
    "circuit = Circuit(qubits=qubits, depth=depth, shots=shots)\n",
    "logger.info(f\"Quantum Circuit erstellt mit {qubits} Qubits und Tiefe {depth}\")\n",
    "\n",
    "# Zeige Eigenschaften des Circuits\n",
    "print(f\"Circuit-Eigenschaften:\")\n",
    "print(f\"Qubits: {circuit.qubits}\")\n",
    "print(f\"Tiefe: {circuit.depth}\")\n",
    "print(f\"Shots: {circuit.shots}\")\n",
    "\n",
    "# Visualisiere den Circuit\n",
    "fig = circuit.plot_circuit()\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.title(\"Quantum Circuit mit L-Gates-Struktur\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Darstellung der L-Gate-Struktur\n",
    "\n",
    "Lass uns die L-Gate-Struktur genauer betrachten. Jedes L-Gate folgt diesem Muster:\n",
    "\n",
    "1. TP0 (Trainingsphase 0) - P-Gate mit trainierbarem Parameter\n",
    "2. IP0 (Inputphase 0) - P-Gate mit Inputparameter\n",
    "3. H - Hadamard-Gate\n",
    "4. TP1 (Trainingsphase 1) - P-Gate mit trainierbarem Parameter\n",
    "5. IP1 (Inputphase 1) - P-Gate mit Inputparameter\n",
    "6. H - Hadamard-Gate\n",
    "7. TP2 (Trainingsphase 2) - P-Gate mit trainierbarem Parameter\n",
    "8. IP2 (Inputphase 2) - P-Gate mit Inputparameter\n",
    "\n",
    "Wir visualisieren dies mit einem einfachen Qubit für bessere Übersichtlichkeit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstelle einen einfachen L-Gate-Circuit für die Visualisierung\n",
    "l_gate_circuit = QuantumCircuit(1)\n",
    "\n",
    "# Füge ein L-Gate hinzu\n",
    "# TP0 → IP0 → H → TP1 → IP1 → H → TP2 → IP2\n",
    "l_gate_circuit.p(0.1, 0)  # TP0 - Trainingsphase 0\n",
    "l_gate_circuit.p(0.2, 0)  # IP0 - Inputphase 0\n",
    "l_gate_circuit.h(0)       # H   - Hadamard\n",
    "l_gate_circuit.p(0.3, 0)  # TP1 - Trainingsphase 1\n",
    "l_gate_circuit.p(0.4, 0)  # IP1 - Inputphase 1\n",
    "l_gate_circuit.h(0)       # H   - Hadamard\n",
    "l_gate_circuit.p(0.5, 0)  # TP2 - Trainingsphase 2\n",
    "l_gate_circuit.p(0.6, 0)  # IP2 - Inputphase 2\n",
    "\n",
    "# Visualisiere den L-Gate-Circuit\n",
    "plt.figure(figsize=(14, 3))\n",
    "circuit_drawer(l_gate_circuit, output='mpl')\n",
    "plt.title(\"Einzelnes L-Gate\")\n",
    "plt.show()\n",
    "\n",
    "# Erläuterung des L-Gates\n",
    "print(\"L-Gate-Struktur:\")\n",
    "print(\"1. TP0 (P-Gate): Trainingsphase 0 - trainierbar\")\n",
    "print(\"2. IP0 (P-Gate): Inputphase 0 - fester Input\")\n",
    "print(\"3. H (Hadamard): Superposition erzeugen\")\n",
    "print(\"4. TP1 (P-Gate): Trainingsphase 1 - trainierbar\")\n",
    "print(\"5. IP1 (P-Gate): Inputphase 1 - fester Input\")\n",
    "print(\"6. H (Hadamard): Superposition erzeugen\")\n",
    "print(\"7. TP2 (P-Gate): Trainingsphase 2 - trainierbar\")\n",
    "print(\"8. IP2 (P-Gate): Inputphase 2 - fester Input\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Erstellen von Trainings- und Eingabematrizen\n",
    "\n",
    "Für die Optimierung benötigen wir zwei Arten von Parametern:\n",
    "1. **Trainingsparameter**: Diese werden durch den Optimierer angepasst\n",
    "2. **Eingabeparameter**: Diese bleiben während der Optimierung konstant\n",
    "\n",
    "Beide werden in spezialisierten Matrizen gespeichert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstelle eine Trainingsmatrix\n",
    "# Die Form der Matrix ist [qubits × depth × 3] - für jede TP-Phase eines jeden L-Gates\n",
    "training_matrix = np.random.uniform(0, np.pi/4, (qubits, depth, 3))  # Initialisierung mit kleinen Werten\n",
    "print(f\"Trainingsmatrix-Form: {training_matrix.shape}\")\n",
    "print(\"Trainingsmatrix (erste Zeile):\\n\", training_matrix[0])\n",
    "\n",
    "# Erstelle mehrere Eingabematrizen für verschiedene Klassifikationsaufgaben\n",
    "num_input_matrices = 4\n",
    "input_matrices = []\n",
    "input_matrix_names = []\n",
    "\n",
    "for i in range(num_input_matrices):\n",
    "    # Verwende unterschiedliche Bereiche, um die Matrizen zu unterscheiden\n",
    "    matrix = np.random.uniform(i*0.5, i*0.5 + 2*np.pi, (qubits, depth, 3))\n",
    "    input_matrices.append(matrix)\n",
    "    input_matrix_names.append(f\"Matrix_{i+1}\")\n",
    "    \n",
    "print(f\"\\nErzeugt: {num_input_matrices} Eingabematrizen\")\n",
    "\n",
    "# Visualisiere die Trainings- und Eingabematrizen\n",
    "fig, axes = plt.subplots(1, min(num_input_matrices + 1, 5), figsize=(18, 5))\n",
    "\n",
    "# Zeige die Trainingsmatrix\n",
    "training_flat = training_matrix.reshape(qubits, -1)\n",
    "sns.heatmap(training_flat, ax=axes[0], cmap=\"viridis\", vmin=0, vmax=2*np.pi)\n",
    "axes[0].set_title(\"Trainingsmatrix\")\n",
    "axes[0].set_xlabel(\"Parameter\")\n",
    "axes[0].set_ylabel(\"Qubit\")\n",
    "\n",
    "# Zeige die Eingabematrizen (bis zu 4)\n",
    "for i in range(min(num_input_matrices, 4)):\n",
    "    input_flat = input_matrices[i].reshape(qubits, -1)\n",
    "    sns.heatmap(input_flat, ax=axes[i+1], cmap=\"plasma\", vmin=0, vmax=2*np.pi)\n",
    "    axes[i+1].set_title(f\"Eingabematrix {i+1}\")\n",
    "    axes[i+1].set_xlabel(\"Parameter\")\n",
    "    axes[i+1].set_ylabel(\"Qubit\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Definition von Zielzuständen für jede Eingabematrix\n",
    "\n",
    "Für die Optimierung müssen wir jeder Eingabematrix einen gewünschten Zielzustand zuweisen. In einem Klassifikationsszenario möchten wir, dass der Circuit für verschiedene Eingabematrizen unterschiedliche Zustände am Ende der Messung produziert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wir erstellen für jede Eingabematrix einen eindeutigen Zielzustand\n",
    "target_states = {}\n",
    "\n",
    "# Binärzahlen als Zielzustände\n",
    "for i in range(num_input_matrices):\n",
    "    # Erzeuge eine binäre Darstellung mit der richtigen Anzahl von Bits (qubits)\n",
    "    binary = format(i+1, f'0{qubits}b')\n",
    "    # Stelle sicher, dass der String die richtige Länge hat (gleich qubits)\n",
    "    target_states[input_matrix_names[i]] = binary[-qubits:]\n",
    "\n",
    "print(\"Zugewiesene Zielzustände:\")\n",
    "for matrix_name, state in target_states.items():\n",
    "    print(f\"{matrix_name}: '{state}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Ausführen eines Quantum Circuits und Analyse der Messungen\n",
    "\n",
    "Bevor wir mit der Optimierung beginnen, führen wir einen Circuit mit einer Eingabematrix aus, um zu verstehen, wie die Messungen aussehen und wie weit wir vom Zielzustand entfernt sind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wir wählen die erste Eingabematrix\n",
    "selected_matrix_idx = 0\n",
    "input_matrix = input_matrices[selected_matrix_idx]\n",
    "matrix_name = input_matrix_names[selected_matrix_idx]\n",
    "target_state = target_states[matrix_name]\n",
    "\n",
    "# Erstelle einen neuen Circuit mit den angegebenen Parametern\n",
    "test_circuit = Circuit(qubits, depth, training_phases=training_matrix.tolist(), \n",
    "                       activation_phases=input_matrix.tolist(), shots=1024)\n",
    "\n",
    "# Führe den Circuit aus\n",
    "test_circuit.run()\n",
    "counts = test_circuit.get_counts()\n",
    "\n",
    "# Konvertiere die Counts in Wahrscheinlichkeiten\n",
    "total_shots = sum(counts.values())\n",
    "probabilities = {state: count/total_shots for state, count in counts.items()}\n",
    "\n",
    "# Sortiere die Zustände nach absteigender Wahrscheinlichkeit\n",
    "sorted_states = sorted(probabilities.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Zeige die Ergebnisse\n",
    "print(f\"Ausführung des Circuits mit {matrix_name}\")\n",
    "print(f\"Zielzustand: '{target_state}'\")\n",
    "print(\"\\nTop 5 gemessene Zustände:\")\n",
    "for i, (state, prob) in enumerate(sorted_states[:5]):\n",
    "    print(f\"{i+1}. Zustand '{state}' mit Wahrscheinlichkeit {prob:.4f} \")\n",
    "    if state == target_state:\n",
    "        print(\"   ✓ Entspricht dem Zielzustand!\")\n",
    "    else:\n",
    "        print(\"   ✗ Entspricht nicht dem Zielzustand\")\n",
    "        \n",
    "# Berechne die Wahrscheinlichkeit des Zielzustands\n",
    "target_probability = probabilities.get(target_state, 0)\n",
    "print(f\"\\nWahrscheinlichkeit des Zielzustands '{target_state}': {target_probability:.4f}\")\n",
    "\n",
    "# Visualisiere die Wahrscheinlichkeiten\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Limitiere auf die Top 10 Zustände für bessere Übersichtlichkeit\n",
    "top_states = sorted_states[:10]\n",
    "states = [s[0] for s in top_states]\n",
    "probs = [s[1] for s in top_states]\n",
    "colors = ['green' if s == target_state else 'blue' for s in states]\n",
    "\n",
    "plt.bar(states, probs, color=colors)\n",
    "plt.title(f\"Top 10 Zustände für {matrix_name} (Zielzustand: {target_state})\")\n",
    "plt.xlabel(\"Quantum Zustand\")\n",
    "plt.ylabel(\"Wahrscheinlichkeit\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Implementierung des Optimierungsprozesses\n",
    "\n",
    "Jetzt implementieren wir den Optimierungsprozess, der die Trainingsmatrix optimiert, um die Wahrscheinlichkeit der Zielzustände für jede Eingabematrix zu maximieren.\n",
    "\n",
    "Wir verwenden den Adam-Optimierer, der aus dem LLY-DML Framework importiert wurde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_for_target_state(input_matrix, training_matrix, target_state, iterations=20):\n",
    "    \"\"\"\n",
    "    Optimiert die Trainingsmatrix für eine bestimmte Eingabematrix und einen Zielzustand.\n",
    "    \n",
    "    Args:\n",
    "        input_matrix: Die Eingabematrix für den Circuit\n",
    "        training_matrix: Die initiale Trainingsmatrix\n",
    "        target_state: Der gewünschte Zielzustand als binärer String\n",
    "        iterations: Anzahl der Trainingsiterationen\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (optimierte Trainingsmatrix, Trainingsgeschichte)\n",
    "    \"\"\"\n",
    "    # Konfigurationsdaten für den Optimizer vorbereiten\n",
    "    optimizer_data = {\n",
    "        \"qubits\": qubits,\n",
    "        \"depth\": depth,\n",
    "        \"learning_rate\": 0.01\n",
    "    }\n",
    "    \n",
    "    # Adam-Optimizer initialisieren\n",
    "    # Der Optimizer arbeitet mit dem ersten Bit des Zielzustands\n",
    "    optimizer = AdamOptimizer(\n",
    "        data=optimizer_data,\n",
    "        training_matrix=training_matrix.flatten().tolist(),\n",
    "        target_state=target_state[0],  # Wir verwenden nur das erste Bit für den Optimizer\n",
    "        learning_rate=0.01,\n",
    "        max_iterations=iterations\n",
    "    )\n",
    "    \n",
    "    # Stelle sicher, dass das tuning_parameters-Attribut existiert\n",
    "    if not hasattr(optimizer, 'tuning_parameters'):\n",
    "        optimizer.tuning_parameters = np.array(training_matrix.flatten().tolist())\n",
    "    \n",
    "    # Trainingsgeschichte initialisieren\n",
    "    history = []\n",
    "    current_training_matrix = training_matrix.copy()\n",
    "    \n",
    "    # Circuit für die Optimierung erstellen\n",
    "    circuit = Circuit(qubits, depth)\n",
    "    \n",
    "    # Training über mehrere Iterationen durchführen\n",
    "    for iteration in range(iterations):\n",
    "        # Aktuelle Trainings- und Eingabematrix in den Circuit einfügen\n",
    "        circuit.place_input_parameters(input_matrix)\n",
    "        circuit.set_train_parameters(current_training_matrix)\n",
    "        circuit.place_train_matrix()\n",
    "        \n",
    "        # Messung hinzufügen und Circuit ausführen\n",
    "        circuit.place_measurement()\n",
    "        counts = circuit.execute_circuit(shots=1024)\n",
    "        \n",
    "        # Optimierungsschritt durchführen\n",
    "        try:\n",
    "            optimized_params, opt_steps = optimizer.optimize()\n",
    "            \n",
    "            # Reshape der optimierten Parameter zurück in die ursprüngliche Form\n",
    "            if len(optimized_params) == qubits * depth * 3:\n",
    "                reshaped_params = np.array(optimized_params).reshape(qubits, depth, 3)\n",
    "                current_training_matrix = reshaped_params\n",
    "            else:\n",
    "                logger.warning(f\"Optimierte Parameter haben falsche Größe: {len(optimized_params)}\")\n",
    "                # Verwende die vorherige Matrix, wenn die neue falsch ist\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Fehler bei der Optimierung: {e}\")\n",
    "            continue\n",
    "            \n",
    "        # Wahrscheinlichkeit des Zielzustands berechnen\n",
    "        probabilities = circuit.get_state_probabilities(counts)\n",
    "        target_prob = probabilities.get(target_state, 0.0)\n",
    "        \n",
    "        # Verlust aus dem letzten Optimierungsschritt\n",
    "        loss = opt_steps[-1][\"loss\"] if opt_steps else 1.0\n",
    "        \n",
    "        # Trainingsschritt in der Historie speichern\n",
    "        history.append({\n",
    "            \"iteration\": iteration + 1,\n",
    "            \"target_probability\": target_prob,\n",
    "            \"loss\": loss\n",
    "        })\n",
    "        \n",
    "        # Status ausgeben\n",
    "        logger.info(f\"Iteration {iteration+1}/{iterations}, \"\n",
    "                    f\"Zielwahrscheinlichkeit: {target_prob:.4f}, Loss: {loss:.4f}\")\n",
    "    \n",
    "    return current_training_matrix, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Durchführung der Optimierung für alle Eingabematrizen\n",
    "\n",
    "Jetzt führen wir die Optimierung für jede Eingabematrix durch und zeichnen den Fortschritt auf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter für die Optimierung\n",
    "training_iterations = 20\n",
    "\n",
    "# Ergebnisse speichern\n",
    "optimization_results = {\n",
    "    \"matrices\": [],\n",
    "    \"initial_states\": {},\n",
    "    \"final_states\": {},\n",
    "    \"training_history\": {}\n",
    "}\n",
    "\n",
    "# Initialisiere die gemeinsame Trainingsmatrix\n",
    "current_training_matrix = training_matrix.copy()\n",
    "\n",
    "# Optimiere für jede Eingabematrix\n",
    "for i, (matrix, name) in enumerate(zip(input_matrices, input_matrix_names)):\n",
    "    target_state = target_states[name]\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Optimierung für {name} mit Zielzustand '{target_state}'\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Führe die Optimierung durch\n",
    "    optimized_matrix, history = optimize_for_target_state(\n",
    "        input_matrix=matrix,\n",
    "        training_matrix=current_training_matrix,\n",
    "        target_state=target_state,\n",
    "        iterations=training_iterations\n",
    "    )\n",
    "    \n",
    "    # Aktualisiere die Trainingsmatrix für die nächste Eingabematrix\n",
    "    current_training_matrix = optimized_matrix\n",
    "    \n",
    "    # Speichere die Ergebnisse\n",
    "    optimization_results[\"matrices\"].append(matrix.tolist())\n",
    "    optimization_results[\"initial_states\"][i] = {\n",
    "        \"name\": name,\n",
    "        \"state\": target_state\n",
    "    }\n",
    "    \n",
    "    # Berechne die finale Wahrscheinlichkeit\n",
    "    final_prob = history[-1][\"target_probability\"] if history else 0.0\n",
    "    optimization_results[\"final_states\"][i] = {\n",
    "        \"state\": target_state,\n",
    "        \"probability\": final_prob,\n",
    "        \"loss\": history[-1][\"loss\"] if history else 1.0\n",
    "    }\n",
    "    \n",
    "    optimization_results[\"training_history\"][i] = history\n",
    "    \n",
    "    print(f\"Optimierung abgeschlossen. Finale Wahrscheinlichkeit für '{target_state}': {final_prob:.4f}\")\n",
    "\n",
    "# Speichere die Ergebnisse in einer JSON-Datei\n",
    "results_dir = os.path.join(notebook_dir, \"results\")\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "results_file = os.path.join(results_dir, f\"optimization_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\")\n",
    "with open(results_file, 'w') as f:\n",
    "    json.dump(optimization_results, f, indent=2)\n",
    "\n",
    "print(f\"\\nOptimierungsergebnisse gespeichert in: {results_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Visualisierung der Optimierungsergebnisse\n",
    "\n",
    "Wir visualisieren den Optimierungsfortschritt und die erreichten Wahrscheinlichkeiten für jede Eingabematrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisiere den Trainingsfortschritt\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "for matrix_idx, history in optimization_results[\"training_history\"].items():\n",
    "    iterations = [entry[\"iteration\"] for entry in history]\n",
    "    probabilities = [entry[\"target_probability\"] for entry in history]\n",
    "    \n",
    "    matrix_name = optimization_results[\"initial_states\"][matrix_idx][\"name\"]\n",
    "    target_state = optimization_results[\"initial_states\"][matrix_idx][\"state\"]\n",
    "    \n",
    "    plt.plot(iterations, probabilities, marker='o', label=f\"{matrix_name} → '{target_state}'\")\n",
    "\n",
    "plt.title(\"Trainingsfortschritt - Wahrscheinlichkeit des Zielzustands\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Wahrscheinlichkeit\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.ylim(0, 1.0)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(results_dir, \"training_progress.png\"), dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# Visualisiere die finalen Wahrscheinlichkeiten\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "matrix_indices = list(optimization_results[\"final_states\"].keys())\n",
    "matrix_names = [optimization_results[\"initial_states\"][idx][\"name\"] for idx in matrix_indices]\n",
    "target_states = [optimization_results[\"initial_states\"][idx][\"state\"] for idx in matrix_indices]\n",
    "final_probs = [optimization_results[\"final_states\"][idx][\"probability\"] for idx in matrix_indices]\n",
    "\n",
    "x = np.arange(len(matrix_indices))\n",
    "plt.bar(x, final_probs, color='coral')\n",
    "plt.title(\"Finale Wahrscheinlichkeiten nach der Optimierung\")\n",
    "plt.xlabel(\"Eingabematrix\")\n",
    "plt.ylabel(\"Wahrscheinlichkeit des Zielzustands\")\n",
    "plt.xticks(x, [f\"{name}\\n'{state}'\" for name, state in zip(matrix_names, target_states)])\n",
    "plt.grid(True, axis='y', alpha=0.3)\n",
    "plt.ylim(0, 1.0)\n",
    "\n",
    "# Füge Werte über den Balken hinzu\n",
    "for i, prob in enumerate(final_probs):\n",
    "    plt.text(i, prob + 0.02, f\"{prob:.3f}\", ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(results_dir, \"final_probabilities.png\"), dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Analyse der optimierten Trainingsmatrix\n",
    "\n",
    "Lass uns die optimierte Trainingsmatrix im Vergleich zur ursprünglichen analysieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vergleiche Original- und optimierte Trainingsmatrix\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Original Trainingsmatrix\n",
    "original_flat = training_matrix.reshape(qubits, -1)\n",
    "sns.heatmap(original_flat, ax=axes[0], cmap=\"viridis\", vmin=0, vmax=2*np.pi)\n",
    "axes[0].set_title(\"Originale Trainingsmatrix\")\n",
    "axes[0].set_xlabel(\"Parameter\")\n",
    "axes[0].set_ylabel(\"Qubit\")\n",
    "\n",
    "# Optimierte Trainingsmatrix\n",
    "optimized_flat = current_training_matrix.reshape(qubits, -1)\n",
    "sns.heatmap(optimized_flat, ax=axes[1], cmap=\"viridis\", vmin=0, vmax=2*np.pi)\n",
    "axes[1].set_title(\"Optimierte Trainingsmatrix\")\n",
    "axes[1].set_xlabel(\"Parameter\")\n",
    "axes[1].set_ylabel(\"Qubit\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(results_dir, \"matrix_comparison.png\"), dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# Berechne die Differenz zwischen den Matrizen\n",
    "diff_matrix = optimized_flat - original_flat\n",
    "plt.figure(figsize=(12, 5))\n",
    "sns.heatmap(diff_matrix, cmap=\"coolwarm\", center=0, vmin=-np.pi, vmax=np.pi)\n",
    "plt.title(\"Parameteränderungen durch Optimierung\")\n",
    "plt.xlabel(\"Parameter\")\n",
    "plt.ylabel(\"Qubit\")\n",
    "plt.colorbar(label=\"Differenz (rad)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(results_dir, \"parameter_changes.png\"), dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Validierung der Optimierung\n",
    "\n",
    "Zum Abschluss validieren wir die Optimierung, indem wir prüfen, ob der Circuit mit der optimierten Trainingsmatrix tatsächlich für jede Eingabematrix den korrekten Zielzustand mit hoher Wahrscheinlichkeit erzeugt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validiere die Optimierung mit der finalen Trainingsmatrix\n",
    "validation_results = []\n",
    "\n",
    "for i, (matrix, name) in enumerate(zip(input_matrices, input_matrix_names)):\n",
    "    target_state = target_states[name]\n",
    "    \n",
    "    # Erstelle einen neuen Circuit mit der optimierten Trainingsmatrix und der aktuellen Eingabematrix\n",
    "    val_circuit = Circuit(qubits, depth, \n",
    "                          training_phases=current_training_matrix.tolist(),\n",
    "                          activation_phases=matrix.tolist(), \n",
    "                          shots=1024)\n",
    "    \n",
    "    # Führe den Circuit aus und erhalte die Counts\n",
    "    val_circuit.run()\n",
    "    counts = val_circuit.get_counts()\n",
    "    \n",
    "    # Berechne die Wahrscheinlichkeiten\n",
    "    total_shots = sum(counts.values())\n",
    "    probabilities = {state: count/total_shots for state, count in counts.items()}\n",
    "    \n",
    "    # Ermittle den wahrscheinlichsten Zustand\n",
    "    most_probable_state = max(probabilities.items(), key=lambda x: x[1])\n",
    "    target_probability = probabilities.get(target_state, 0)\n",
    "    \n",
    "    # Speichere die Ergebnisse\n",
    "    validation_results.append({\n",
    "        \"matrix_name\": name,\n",
    "        \"target_state\": target_state,\n",
    "        \"most_probable_state\": most_probable_state[0],\n",
    "        \"most_probable_prob\": most_probable_state[1],\n",
    "        \"target_probability\": target_probability,\n",
    "        \"is_correct\": most_probable_state[0] == target_state\n",
    "    })\n",
    "\n",
    "# Zeige die Validierungsergebnisse\n",
    "print(\"Validierungsergebnisse:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Matrix':15s} | {'Zielzustand':12s} | {'Wahrscheinlichste':15s} | {'Wahrsch.':8s} | {'Korrekt':8s}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for result in validation_results:\n",
    "    print(f\"{result['matrix_name']:15s} | {result['target_state']:12s} | \"\n",
    "          f\"{result['most_probable_state']:15s} | {result['most_probable_prob']:.4f} | \"\n",
    "          f\"{('✓' if result['is_correct'] else '✗'):8s}\")\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print(f\"Korrekte Klassifikationen: {sum(1 for r in validation_results if r['is_correct'])} von {len(validation_results)}\")\n",
    "\n",
    "# Visualisiere die Validierungsergebnisse\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "matrix_names = [r[\"matrix_name\"] for r in validation_results]\n",
    "target_probs = [r[\"target_probability\"] for r in validation_results]\n",
    "most_probable_probs = [r[\"most_probable_prob\"] for r in validation_results]\n",
    "\n",
    "x = np.arange(len(matrix_names))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(x - width/2, target_probs, width, label='Zielzustand', color='green')\n",
    "plt.bar(x + width/2, most_probable_probs, width, label='Wahrscheinlichster Zustand', color='orange')\n",
    "\n",
    "plt.title(\"Validierung der Optimierung\")\n",
    "plt.xlabel(\"Eingabematrix\")\n",
    "plt.ylabel(\"Wahrscheinlichkeit\")\n",
    "plt.xticks(x, matrix_names)\n",
    "plt.ylim(0, 1.0)\n",
    "plt.grid(True, axis='y', alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "# Füge Textinformationen für jeden Balken hinzu\n",
    "for i, result in enumerate(validation_results):\n",
    "    plt.text(i - width/2, result[\"target_probability\"] + 0.02, \n",
    "             f\"{result['target_probability']:.3f}\", ha='center')\n",
    "    plt.text(i + width/2, result[\"most_probable_prob\"] + 0.02, \n",
    "             f\"{result['most_probable_prob']:.3f}\", ha='center')\n",
    "    \n",
    "    # Zeige den Zielzustand unter dem Balken\n",
    "    plt.text(i, -0.07, f\"Ziel: '{result['target_state']}'\", ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(results_dir, \"validation_results.png\"), dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Zusammenfassung und Schlussfolgerungen\n",
    "\n",
    "In diesem Tutorial haben wir gesehen, wie mit dem LLY-DML Framework die Parameter eines Quantum Circuits mit L-Gates optimiert werden können. Wir haben folgende Schritte durchgeführt:\n",
    "\n",
    "1. **Circuit-Erstellung**: Wir haben einen Circuit mit L-Gates erstellt, der aus einer spezifischen Abfolge von Trainingsphasen, Inputphasen und Hadamard-Gates besteht.\n",
    "\n",
    "2. **Datenvorbereitung**: Wir haben Trainings- und Eingabematrizen erstellt und jedem Input einen eindeutigen Zielzustand zugeordnet.\n",
    "\n",
    "3. **Optimierung**: Mit dem Adam-Optimierer haben wir die Trainingsmatrix optimiert, um die Wahrscheinlichkeit zu maximieren, dass der Circuit bei Eingabe einer bestimmten Matrix den zugeordneten Zielzustand liefert.\n",
    "\n",
    "4. **Validierung**: Wir haben überprüft, ob der optimierte Circuit tatsächlich für die verschiedenen Eingaben die richtigen Zielzustände mit hoher Wahrscheinlichkeit erzeugt.\n",
    "\n",
    "5. **Visualisierung**: Wir haben die Ergebnisse visualisiert, um ein besseres Verständnis des Optimierungsprozesses zu erhalten.\n",
    "\n",
    "Dieser Prozess demonstriert die Grundprinzipien des Quantum Machine Learnings, indem die Parameter eines Quantum Circuits trainiert werden, um spezifische Ausgaben für bestimmte Eingaben zu erzeugen - ähnlich zu klassischen neuronalen Netzen, aber mit den einzigartigen Eigenschaften von Quantencomputern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Weitere Schritte und Verbesserungen\n",
    "\n",
    "Hier sind einige mögliche Erweiterungen und Verbesserungen für zukünftige Untersuchungen:\n",
    "\n",
    "1. **Mehr Optimierer**: Implementieren und vergleichen Sie verschiedene Optimierungsalgorithmen wie SGD, Momentum, RMSprop und Nadam.\n",
    "\n",
    "2. **Komplexere Daten**: Verwenden Sie reale Datensätze und transformieren Sie diese in geeignete Inputmatrizen.\n",
    "\n",
    "3. **Hyperparameter-Tuning**: Experimentieren Sie mit verschiedenen Parametern wie Lernrate, Anzahl der Qubits, Tiefe oder Anzahl der Iterationen.\n",
    "\n",
    "4. **Fehleranalyse**: Untersuchen Sie, warum manche Zielzustände leichter zu optimieren sind als andere.\n",
    "\n",
    "5. **Circuit-Variationen**: Experimentieren Sie mit verschiedenen Gate-Strukturen und -Kombinationen.\n",
    "\n",
    "Das LLY-DML Framework bietet eine flexibel erweiterbare Plattform für diese und weitere Experimente im Bereich des Quantum Machine Learnings."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}